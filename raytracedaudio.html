<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <!-- SEO Meta Tags - ADDED -->
    <meta name="description" content="My blogs">
    <meta name="author" content="Alex V">
    
    <!-- Open Graph - ADDED -->
    <meta property="og:title" content="Blogs">
    <meta property="og:description" content="Blogs about projects">
    <meta property="og:image" content="Content/Images/gllogo.PNG">
    
    <link href="Content/highlight/styles/atom-one-dark.min.css" rel="stylesheet" />
    
    <title>Blog page</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="icon" type="image/png" href="favicon.png">
</head>

<body>
    <div class="swirl-line"></div>
    <div class="swirl-line"></div>

    <!-- ADDED: Secret easter egg for consistency -->
    <div class="hover-image secret3">
        <img src="Content/Images/ddslogo.png" alt="Hidden DDS logo easter egg">
    </div>

    <!-- CHANGED: Added social links -->
    <nav class="main-nav" aria-label="Main navigation">
        <ul class="nav-links">
            <li><a href="index.html">Projects</a></li>
            <li><a href="about.html">About</a></li>
            <li><a href="blog.html">Blog</a></li>
        </ul>
        
        <div class="social-links">
            <a href="https://www.linkedin.com/in/av3xv/" 
               target="_blank" 
               rel="noopener noreferrer" 
               class="social-icon"
               aria-label="LinkedIn Profile">
                <img src="Content/Images/LinkedinIcon.svg" alt="LinkedIn">
            </a>
            <a href="https://av03.itch.io/" 
               target="_blank" 
               rel="noopener noreferrer" 
               class="social-icon"
               aria-label="Itch.io Profile">
                <img src="Content/Images/ItchIcon.svg" alt="Itch.io">
            </a>
        </div>
    </nav>

    <!-- 
    CHANGED: Removed commented code, kept image version
    WHY: Cleaner code, image title is more visually interesting
    -->
    <div class="page-header">
        <h1>Ray traced audio</h1> <!-- CHANGE this text for your title -->
    </div>

    <!-- CHANGED: div to main for semantic HTML -->
    <main class="content-border">
        <div class="border-top"></div>
        <div class="border-content">
            <!-- 
            CHANGED: <b> to <h2> for semantic headings
            WHY: Better SEO, accessibility, and document structure
            -->
            <h3>About</h3>
            <p>
                I will be talking about my implementation of ray traced audio in c++. This is my second year project at BUAS.
                We had to choose a topic and i found audio a interesting topic. I saw a video about someone implementing
                ray traced audio in his own engine and i started looking and couldnt find much about ray traced audio,
                so i thought i will give it a try and have nice experience and challenge.
            </p>

            <figure class="content-row">
                <div class="project-info-box">
            
                <img src="Content/Images/wav-sound-format.png" 
                     alt="Wav file format"
                     loading="lazy">
                </div>

                <figcaption class="text-beside">
                    <h3>The audio file</h3>
                    <!-- CHANGED: <b> to <h3> -->
                    <p>
                        I will be using the WAV file for this project. I am using this because WAV files are uncompressed and with that easy to modify,
                        without risking to lose quality. With the first part of the WAV file reader, We open the wav file by loading it in a hexadecimal format.
                        and making the WAVE variables where the data will be stored.
                    </p>

                <pre><code class="language-cpp">
std::ifstream wavfile(_filePath, std::ios::binary);

if (wavfile.is_open())
{
	// Read the WAV header
	char chunk_ID[4];              //  4  riff_mark[4];
	uint32_t chunk_size;           //  4  file_size;
	char format[4];                //  4  wave_str[4];

	char sub_chunk1_ID[4];         //  4  fmt_str[4];
	uint32_t sub_chunk1_size;      //  4  pcm_bit_num;
	uint16_t audio_format;         //  2  pcm_encode;
	uint16_t num_channels;         //  2  sound_channel;
	uint32_t sample_rate;          //  4  pcm_sample_freq;
	uint32_t byte_rate;            //  4  byte_freq;
	uint16_t block_align;          //  2  block_align;
	uint16_t bits_per_sample;      //  2  sample_bits;
	char sub_chunk2_ID[4];		   //  4  data_str[4];
	uint32_t sub_chunk2_size = 0;  //  4  sound_size;
                </code></pre>

                </figcaption>
            </figure>

            <p>
                We can then read the wav file's chunk ID to check if the file is of type RIFF and then read the chunk_size that stores the size of the file
                and format. And then we read the format that should store the name WAVE, we
                can then check if it's a WAVE file. Together they check if the WAVE file is correct.
            </p>

                <pre><code class="language-cpp">
wavfile.read(chunk_ID, 4);

if (std::string(chunk_ID, 4) == "RIFF")
{
    wavfile.read(reinterpret_cast<char*>(&chunk_size), 4);
    wavfile.read(reinterpret_cast<char*>(&format), 4);

    if (std::string(format, 4) == "WAVE")
                </code></pre>

            <p>
                When having read through the whole file we can then use sub_chunk2_size to get the number of samples to use later to resize the vector
                to store the audio samples.
            </p>

            <pre><code class="language-cpp">
std::vector<int16_t> audio_data(sub_chunk2_size / sizeof(int16_t));
wavfile.read(reinterpret_cast<char*>(audio_data.data()), sub_chunk2_size);
wavfile.close();  // Close audio file
            </code></pre>

            <figure class="content-row">
                <div class="page-game rivv1">
                    <video controls width="400" preload="metadata" aria-label="Panning">
                        <source src="Content/Videos/Panning.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>

                <figcaption class="text-beside">
                    <h3>Stereo Panning</h3>
                    <p>
                        Stereo panning allowes the listener to determine of where a sound is coming from.
                        For this I used a simple algorithm that calculates the direction and rotation between the listener and the audio source.
                        This way it calculates a panning value thats between -1,0,1. -1 means sound is on your left 0 in the center 1 sound is coming from
                        the right. this is being used to asjust the volume of the left and right channels, with this you create the effect that it is
                        directional.
                    </p>
                </figcaption>
            </figure>

                <pre><code class="language-cpp">
glm::vec3 dir = audiosourcePosition - listenerPosition;

glm::vec3 norDir = glm::normalize(dir);
glm::vec3 norRight = glm::normalize(glm::cross(listenerForward, glm::vec3(0, 1, 0)));

float pan = glm::dot(norDir, norRight);

float angle = (pan + 1.0f) * PIs * 0.25;
_left = cosf(angle);
_right = sinf(angle);
                </code></pre>

            <!-- 
            WHY: Side-by-side video and explanation
            WHAT: Flexbox layout with video left, text right
            CHANGED: Added figure/figcaption for semantic HTML
            -->
            <figure class="content-row">
                <div class="page-game rivv1">
                    <video controls width="400" preload="metadata" aria-label="Occlusion of sound">
                        <source src="Content/Videos/Occlusion.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <!-- CHANGED: Added preload, aria-label, fallback text -->
                </div>

                <figcaption class="text-beside">
                    <h3>Occlusion</h3>
                    <!-- CHANGED: <b> to <h3> -->
                    <p>
                        To have more realistic audio I added occlusion to the DSP(Digital signal processor) chain.
                        This reduces the audio volume and applies a low pass filter to create the muffeld effect when their is a object
                        in between the listener and the audio source.
                        This works by shooting rays towards the listener and checking if they collided with the environment. If a ray collides
                        with a object it will get the objects alpha factor and uses that later for the low pass filter.
                    </p>
                </figcaption>
            </figure>

            <figure class="content-row">
                <pre><code class="language-cpp">
glm::vec3 dirToListener = listenerPosition - source.position;
SoundRay occlusionRay(source.position, dirToListener, 0.0f);
OcclusionTrace(occlusionRay, listenerPosition, audioManager->GetListener().listenerRadius);

if (hitGeometry)
{
	// Get material properties and set absorption
	int primIdx = tbRay.hit.prim;
	int objIDx = GetIDFromTriangleIdx(primIdx);

	for (auto& aMesh : audioManager->GetAudioMeshes())
	{
		if (objIDx == aMesh.meshTag.ID)
		{
			_sRay.alpha = aMesh.soundMaterial.absorpValue;
			break;
		}
	}

	return;
}

//Low pass filter for left sample
prevLeft = alpha * dryLeft + (1.0f - alpha) * prevLeft;
                </code></pre>

                <figcaption class="text-beside">
                    <p>
                        I am using a first order IIR(Infinte Impulse Response) low pass filter also known as a one pole filter.
                        The low pass filter smooths high frequency changes, the output is created by using a alpha with the current sample and the previoous
                        sample. The alpha is the factor for how much smoothing is applied to the audio. Normally the alpha is caluclated based on the
                        cutoff frequency but for my case it is a single value that the user can define for each object.
                        This works by giving a object a custom soundmaterial that stores a alpha value, when creating that material you define it a
                        value that fits for the object, with this you can be very specific on how much occlusion happens for each material type.
                    </p>
                </figcaption>
            </figure>

            <figure class="content-row">
                <div class="page-game rivv1">
                    <video controls width="400" preload="metadata" aria-label="Echo in sound">
                        <source src="Content/Videos/Echo.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <!-- CHANGED: Added preload, aria-label, fallback text -->
                </div>

                <figcaption class="text-beside">
                    <h3>Echo</h3>
                    <!-- CHANGED: <b> to <h3> -->
                    <p>
                        To add some more realism I implened echo's into the DSP chain. Echos are created using a circular buffer delay line with a
                        distance based decay. For this you need a extra buffer for the echo and a read/write index.
                    </p>
                </figcaption>
            </figure>

            <figure class="content-row">
                <pre><code class="language-cpp">
    //343 speed of sound in air 20 degreess
	float delayInMS = (ray.totalDistance * 1000) / 343;

	//Echo
	int readIdx = audioSource.echoWritePos * 2;
	float leftEcho = audioSource.echoBuffer[readIdx];
	float rightEcho = audioSource.echoBuffer[readIdx + 1];
                </code></pre>

                <figcaption class="text-beside">
                    <p>
                        In the first part of echo I get the reading position and getting the previous sample of the echobuffer.
                        This is later used again with the current sample and a decay. 
                    </p>
                </figcaption>
            </figure>

            <figure class="content-row">
                <pre><code class="language-cpp">
	float decay = 0.4f;
	float distance = glm::clamp(ray.totalDistance * 0.01f, 0.0f, 1.0f);
	float echoDecay = decay * (1.0f - distance * 0.3f);
                </code></pre>

                <figcaption class="text-beside">
                    <p>
                        The second part calculates the decay of the echo. This is done with a default decay factor and applying it to
                        the distance the ray has traveled, this results in a decay factor thats based on how far the sound ray has traveled.
                    </p>
                </figcaption>
            </figure>

            <figure class="content-row">
                <pre><code class="language-cpp">
	_leftOut = (_leftDry * DRY_MIX) + (leftEcho * WET_MIX);
	_rightOut = (_rightDry * DRY_MIX) + (rightEcho * WET_MIX);

	audioSource.echoBuffer[readIdx] = _leftDry + (leftEcho * echoDecay);
	audioSource.echoBuffer[readIdx + 1] = _rightDry + (rightEcho * echoDecay);

	audioSource.echoWritePos = (audioSource.echoWritePos + 1) % audioSource.echoDelaySamples;
                </code></pre>

                <figcaption class="text-beside">
                    <p>
                        The third and last part is the circular buffer part. This adds a echo sample with a dry and wet factor to the output buffer.
                        Then a new sample based on the output sample and a decayed echo sampleis are stored in the echo buffer. As last the write position
                        is to the next position. The dry factor determines how much of the orginal audio is passed to the output.
                        Wet factor determines how much of the echo is passed to the output. 
                    </p>
                </figcaption>
            </figure>

            <figure class="content-row">
                <figcaption class="text-beside">
                    <h3>Ray tracing</h3>
                    <p>
                        Then the ray tracing part of the audio. Every frame a x amount of rays are shot from the audio source in a spherical direction
                        calculated with a fibonaccisphere formula. This goes through the tracing logic and eventually if the sound ray has reached the
                        listener it will add its energy and distance traveld to a new ray.
                    </p>
                </figcaption>

                <pre><code class="language-cpp">
for (int i = 0; i < source.rayAmount; i++)
{
	SoundRay ray(source.position, CalculateFibonacciSphere(source.rayAmount, i, 1.0f), 0.0f);
	// ... tracing logic

	if (ray.reachedListener)
	{
		sRay.energy += ray.energy;
		sRay.totalDistance += ray.totalDistance;
	}
                </code></pre>
            </figure>

            <figure class="content-row">
                <pre><code class="language-cpp">
    for (SoundRay& transRay : transmittedRays)
    {
	std::vector<SoundRay> nestedTrans;
	Trace(transRay, 0, listenerPosition, lr, nestedTrans, RayType::Transmitted);
	if (transRay.reachedListener)
	{
		if (!ray.reachedListener)
		{
			transAmount++;
			sRay.energy += transRay.energy;
		}
	sRay.totalDistance += transRay.totalDistance;
	}
    }
}
                </code></pre>

                <figcaption class="text-beside">
                    <h4>Transmission</h4>
                    <p>
                        with the rays, each ray when it hits a object it will also create a new transmission ray with reduced energy.
                        The reduction of the sound energy is like reallive where when sound hits a object a part of it goes trough the object,
                        part of it reflects and a part of it gets absorpt. this ray goes through the object
                        to increase realism for when a sound is behind a object.
                    </p>
                </figcaption>
            </figure>

            <figure class="content-row">
                <pre><code class="language-cpp">
float invNumRays = 1.0f / source.rayAmount;
sRay.totalDistance *= invNumRays;
invNumRays = 1.0f / (source.rayAmount + transAmount);
sRay.energy *= invNumRays;
                </code></pre>

                <figcaption class="text-beside">
                    <p>
                        eventually when the tracing is done the average of all the values are taken and supplied to a single ray that gets stored,
                        so later it can be used to calculated the different DSP chain effects.
                    </p>
                </figcaption>
            </figure>

            <figure class="content-row">
                <figcaption class="text-beside">
                    <p>
                        For the intersection of the rays with the enviroment I am using TinyBVH(make into link) by Jacco Bikker.
                        this gets me the data to calculate the intersection point of where the sound ray collides with a object.`
                        With the intersection point and the starting point i can then calculate a point on that line to check, how far
                        the listener is from the ray, this makes it able to detect the listener.
                    </p>
                </figcaption>

                <pre><code class="language-cpp">
// ... intersection logic

	//Calculate intersection point
	glm::vec3 prevIntersectionPoint = _sRay.origin;
	glm::vec3 intersectionPoint = _sRay.origin + _sRay.distance * _sRay.direction;

	//Calculate distance from point on ray line and player position
	glm::vec3 pointline = GetNearestPointOnLineSegment(_listenerPos, intersectionPoint, prevIntersectionPoint);
	
	float ilDis = glm::distance(pointline, _listenerPos);
	if (ilDis < _listenerRadius)//Check if ray has hit player
	{
		_sRay.reachedListener = true;
			if (_depth == 0)
				_sRay.hasDirectPath = true;
		return;
	}
                </code></pre>
            </figure>

            <figure class="content-row">
                <figcaption class="text-beside">
                    <p>
                        The next part is if the sound rays did not reach the player directly find the object it hitted and getting its sound material.
                        after that it checks if the sound ray can still go trough objects if so than a new ray is created.
                    </p>
                </figcaption>

                <pre><code class="language-cpp">
// ... getting object material logic

	if (_sRay.transmissionAmount > 0 && _sRay.energy > 0.005f)
	{
		SoundRay transmittedRay = CreateTransmissionSoundRay(_sRay, intersectionPoint, soundM);

	    if (transmittedRay.energy > 0.1f)
		transmittedRays.push_back(transmittedRay);
	}
                </code></pre>
            </figure>

            <figure class="content-row">
                <figcaption class="text-beside">
                    <p>
                        the rays energy gets reduced depending on the absorption value of the material.
                        The ray then calcualts a reflection that is used to shoot a new ray again by calling the Trace function again.
                        With this you have a recursive behavior so long the sound ray has enough energy.
                    </p>
                </figcaption>

                <pre><code class="language-cpp">
_sRay.energy *= (1.0f - soundM.absorpValue);

	//Calculate reflection
	glm::vec3 reflection = _sRay.direction - 2.0f * normals[primIdx] * glm::dot(_sRay.direction, normals[primIdx]);

	_sRay.origin = intersectionPoint + reflection * EPSILON;
	_sRay.direction = reflection;

	Trace(_sRay, _depth + 1, _listenerPos, _listenerRadius, transmittedRays, _rt);
                </code></pre>
            </figure>

            <h3>Conclusion</h3>
            <p>
                These are currently the features implemented into my ray traced audio tool. Altough not everything is implemented yet In the future
                I plan to add more features and also improve the performance and user friendlyniss, to make it a proper tool that can be used in any project.
                I learned a lot from this project, mainly about how sound works and how it can be applied into programming. and hope you also learned something
                about it. Thank you for reading my blog.
                ps did you found the ducky :)
            </p>

            <br>

            <h4>References</h4>
            <p>
                MAEK INTO linkies
                https://en.wikipedia.org/wiki/WAV<br>
                https://learncplusplus.org/learn-how-to-read-the-wav-waveform-audio-file-format-in-c/
                https://hackaudio.com/tutorial-courses/learn-audio-programming-table-of-contents/digital-signal-processing/echo-effects/feed-back-echo/
                A good website to look up absoroption values for different materials:
                https://www.acoustic-supplies.com/absorption-coefficient-chart/
                https://jacco.ompf2.com/2025/01/24/tinybvh-manual-basic-use/
            </p>

        </div>
        <div class="border-bottom"></div>
    </main>
    <script src="Content/highlight/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
</body>
</html>